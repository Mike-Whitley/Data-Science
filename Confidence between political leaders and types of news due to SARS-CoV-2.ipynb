{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMZyBUJqk8AWO8m29QQx2oZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SIlp8XDGikWS","colab_type":"text"},"source":["Summary \n","To what extent has there been a change in confidence between political leaders and types of news due to SARS-CoV-2. The subset of GDELT data that will be used to answer this question is the Event table with Actor1Name, Actor1CountryCode and EventCode as the columns. Market basket analysis including A-priori algorithm will be used and the intended result will return the actor and event tuple with confidence for each record this will include all countries and those specifically defined in the rules below. The data will be within the date ranges 31/08/19 - 30/12/19 for pre SARS-CoV-2 and 31/12/19 - 31/04/20 for post SARS-CoV-2. This result will allow me to make a simple comparison between the two date ranges and in turn answer the question.\n"]},{"cell_type":"markdown","metadata":{"id":"M1GCwEbSjIZw","colab_type":"text"},"source":["Event table with Actor1Name, Actor1CountryCode and EventCode as the columns31/08/19 - 30/12/19 for pre SARS-CoV-2 and 31/12/19 - 31/04/20 for post SARS-CoV-2. "]},{"cell_type":"markdown","metadata":{"id":"mgeOIj9ujKHN","colab_type":"text"},"source":["#Imports and installs required for project"]},{"cell_type":"code","metadata":{"id":"1X0OTFOxjdmN","colab_type":"code","outputId":"df028f11-12aa-47e3-f7c3-302f8bdb94c4","executionInfo":{"status":"ok","timestamp":1589496011480,"user_tz":-720,"elapsed":87936,"user":{"displayName":"Mike W","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQR4N0_-ODVQsJephHo5QPiWBc7uFJVAe8iEvi=s64","userId":"10194701315001433921"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#library and code setup - retrived from Sample GDELT starter only.ipynb \"Written by James Atlas\"\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!pip install -q pyspark\n","!pip install newspaper3k\n","!pip install gdelt\n","\n","#library and code setup\n","import pyspark, os\n","from pyspark import SparkConf, SparkContext\n","os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n","os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64/\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 217.8MB 60kB/s \n","\u001b[K     |████████████████████████████████| 204kB 47.6MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting newspaper3k\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n","\u001b[K     |████████████████████████████████| 215kB 4.7MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n","Collecting tinysegmenter==0.3\n","  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n","Collecting cssselect>=0.9.2\n","  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n","Collecting jieba3k>=0.35.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n","\u001b[K     |████████████████████████████████| 7.4MB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n","Collecting feedparser>=5.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n","\u001b[K     |████████████████████████████████| 194kB 46.3MB/s \n","\u001b[?25hCollecting feedfinder2>=0.0.4\n","  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n","Collecting tldextract>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n","Collecting requests-file>=1.4\n","  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n","Building wheels for collected packages: tinysegmenter, jieba3k, feedparser, feedfinder2\n","  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=fcd9c8f64ddfe75a334e837aec09ecc1099cdd62f10f78506289ab320e6230e0\n","  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=eaf27207e4ad45d488fec6f3dd08ea6fb38a4c43c8283e47ab150ca89088dc86\n","  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n","  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=ce897104b976cb3a7358d4d95033ca005e7f70b255a90345668e545eeaacac7d\n","  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=debfda9ba199d04d099e0373f8eacc82b38020f5ec9998a83f42c43e475b213c\n","  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n","Successfully built tinysegmenter jieba3k feedparser feedfinder2\n","Installing collected packages: tinysegmenter, cssselect, jieba3k, feedparser, feedfinder2, requests-file, tldextract, newspaper3k\n","Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n","Collecting gdelt\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/f9/a3d5111c8f17334b1752c32aedaab0d01ab4324bf26417bd41890d5b25d0/gdelt-0.1.10.6.1-py2.py3-none-any.whl (773kB)\n","\u001b[K     |████████████████████████████████| 778kB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdelt) (2.23.0)\n","Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from gdelt) (1.0.3)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from gdelt) (2.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gdelt) (1.18.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (2.9)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->gdelt) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->gdelt) (1.12.0)\n","Installing collected packages: gdelt\n","Successfully installed gdelt-0.1.10.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oMNdj1oXjeX0","colab_type":"code","colab":{}},"source":["#start spark local server - retrived from Sample GDELT starter only.ipynb \"Written by James Atlas\"\n","import sys, os\n","from operator import add\n","import time\n","\n","os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n","\n","import pyspark\n","from pyspark import SparkConf, SparkContext\n","\n","#connects our python driver to a local Spark JVM running on the Google Colab server virtual machine\n","try:\n","  conf = SparkConf().setMaster(\"local[*]\").set(\"spark.executor.memory\", \"1g\")\n","  sc = SparkContext(conf = conf)\n","except ValueError:\n","  #it's ok if the server is already started\n","  pass\n","\n","def dbg(x):\n","  \"\"\" A helper function to print debugging information on RDDs \"\"\"\n","  if isinstance(x, pyspark.RDD):\n","    print([(t[0], list(t[1]) if \n","            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])\n","           if isinstance(t, tuple) else t\n","           for t in x.take(100)])\n","  else:\n","    print(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjnZc-U4j74C","colab_type":"code","outputId":"664f84f6-0f30-466b-cc4a-f8231ee6ec32","executionInfo":{"status":"ok","timestamp":1589496025453,"user_tz":-720,"elapsed":101899,"user":{"displayName":"Mike W","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQR4N0_-ODVQsJephHo5QPiWBc7uFJVAe8iEvi=s64","userId":"10194701315001433921"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# these remove files from previous runs - retrived from Sample GDELT starter only.ipynb \"Written by James Atlas\"\n","!rm -rf articles\n","!rm *.csv"],"execution_count":3,"outputs":[{"output_type":"stream","text":["rm: cannot remove '*.csv': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y4LXHDcc67Vu","colab_type":"text"},"source":["Function to generate all the time slots required to check hourly via api"]},{"cell_type":"code","metadata":{"id":"45jPG7Bw65wu","colab_type":"code","colab":{}},"source":["def create_time():\n","  \"\"\"basic loop to get all the times we need to call later \"\"\"\n","  time_loop = []\n","  hour_start = 000000\n","  for i in range(0, 24):\n","          \"\"\"simple loops to calculate all base time for hours for api call per hour \"\"\"\n","          if i == 0:\n","                  time_loop.append((\"000000\", \"005959\"))\n","              \n","          elif i <= 9:\n","              hour_start = 0\n","              hour_start =  hour_start + i * 10000\n","              time_loop.append((\"0\" + str(hour_start), \"0\" + str(hour_start+5959)))\n","\n","          else:\n","              hour_start = 0\n","              hour_start =  hour_start + i * 10000\n","              time_loop.append((str(hour_start),str(hour_start+5959)))\n","  return time_loop"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"itD_07w9kL6l","colab_type":"code","colab":{}},"source":["import timeit\n","import requests\n","import csv \n","import pandas as pd\n","\n","def Api_Hourly_call(date_to_download, time_loop, queryURLbase, filename):\n","  \"\"\"Makes an api call 24 times for each hour of the day then merges them into a single csv file.\n","  checks for duplicates of articles/reposts and condenses in a single file takes about 30 mins all up for 4 months of records \"\"\"\n","\n","  from more_itertools import unique_everseen\n","  #run through the day hour by hour to get all the csv files downloaded and combined \n","  for d_time in time_loop:\n","    startdate = date_to_download + d_time[0] #0 hour 0 min 0 sec (start of day)\n","    enddate = date_to_download + d_time[1]#23 hour 59 min 59 sec (end of day)\n","    query = queryURLbase + \"&startdatetime=\" + startdate + \"&enddatetime=\" + enddate\n","\n","    #downloads the new csv file for the specificed hour\n","    req = requests.get(query)\n","    url_content = req.content\n","    csv_file = open('downloaded.csv', 'wb')\n","    csv_file.write(url_content)\n","    csv_file.close()\n","\n","    try:\n","      #combines the csv files and checks for duplicates \n","      df1 = pd.read_csv(filename)\n","      df2 = pd.read_csv('downloaded.csv')\n","      df_row = pd.concat([df1, df2])\n","      df_row.drop_duplicates(subset=None, inplace=True) #sometimes get duplicates on the hour this removes them\n","      df_row.to_csv(filename, index=False)\n","\n","\n","      # print(\"start time: \" + d_time[0] + \" to end time: \" + d_time[1] + \" has been added to output file.\")\n","      # print(query)\n","      # print(\"added:\")\n","    except:\n","      # print(\"Empty File no records at this time omitted from combination: \", query)\n","      pass\n","\n","\n","\n","#code to time a function\n","# start = timeit.default_timer()\n","# stop = timeit.default_timer()\n","# print('Time taken to download all files: ', stop - start) \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZWjHxvU7Zsw","colab_type":"text"},"source":["#Generates the list of dates needed for API in correct format"]},{"cell_type":"code","metadata":{"id":"iPcmZACy7cfE","colab_type":"code","colab":{}},"source":["def long_enough(date):\n","  \"\"\"helper function for getting correct date format \"\"\"\n","  if date <= 9:\n","    date = \"0\" + str(date)\n","  return date\n","\n","def get_date_range(date_start, date_end):\n","  \"\"\"Take two dates input start date and end date and generates all the dates in between in usable format for API \"\"\"\n","  d_lst = []\n","  date_range = [d_lst.append(str(x.year)+ str(long_enough(x.month)) + str(long_enough(x.day))) for x in pd.date_range(date_start, date_end)]\n","  return d_lst"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aD-sIdLHcnJO","colab_type":"text"},"source":["#NZ 31/01/19 to 30/04/2020 keywords inc corona virus and covid 19 etc.."]},{"cell_type":"code","metadata":{"id":"n0VUUeMBHgod","colab_type":"code","outputId":"9eaefa40-b8df-48d9-8032-a1a302e59d29","executionInfo":{"status":"error","timestamp":1589441208876,"user_tz":-720,"elapsed":1119844,"user":{"displayName":"Mike W","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQR4N0_-ODVQsJephHo5QPiWBc7uFJVAe8iEvi=s64","userId":"10194701315001433921"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#gets the data for during the corona virus period average run time 30 - 35 mins for 4 months of records hourly \n","import pandas as pd\n","import csv\n","import timeit\n","start = timeit.default_timer()\n","\n","#API I want to download CSV file from includes my key words\n","URL_OF_REQUEST = \"https://api.gdeltproject.org/api/v2/doc/doc?format=html&query=(%22Coronavirus%22%20OR%20%22Covid-19%22%20OR%20%22Sars-Cov-19%22%20OR%20%22chinese%20virus%22%20OR%20%22chinese%20flu%22%20OR%20%22SARS-CoV-2%22)%20sourcecountry:NZ&mode=artlist&maxrecords=250&format=csv&sort=hybridrel\"\n","#Create all the times in 24 hours\n","time_loop = create_time()\n","#start a counter to see remaining days left to process\n","count = 0\n","#get the date range for the specific dates we want in API format\n","date_range = get_date_range('2019-12-31', '2020-04-30') # 2019-12-31\n","#set the file name we want our condensed file as \n","filename = \"Coronavirus_records_unfiltered.csv\"\n","#just a simple variable to see how many days in total we are doing to see how many left\n","total = len(date_range)\n","#generate an empty csv file as to not error at start\n","df = pd.DataFrame(list())\n","df.to_csv(filename)\n","\n","#loop through all dates calling the API on every date combining each time\n","for date in date_range:\n","  Api_Hourly_call(date, time_loop, URL_OF_REQUEST, filename)\n","  print(\"completed date: \", date)\n","  count = count + 1\n","  print(str(count) + \" of \" + str(total) + \" remaining\")\n","stop = timeit.default_timer()\n","print('Time taken to download all files: ', stop - start) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["completed date:  20191231\n","1 of 122 remaining\n","completed date:  20200101\n","2 of 122 remaining\n","completed date:  20200102\n","3 of 122 remaining\n","completed date:  20200103\n","4 of 122 remaining\n","completed date:  20200104\n","5 of 122 remaining\n","completed date:  20200105\n","6 of 122 remaining\n","completed date:  20200106\n","7 of 122 remaining\n","completed date:  20200107\n","8 of 122 remaining\n","completed date:  20200108\n","9 of 122 remaining\n","completed date:  20200109\n","10 of 122 remaining\n","completed date:  20200110\n","11 of 122 remaining\n","completed date:  20200111\n","12 of 122 remaining\n","completed date:  20200112\n","13 of 122 remaining\n","completed date:  20200113\n","14 of 122 remaining\n","completed date:  20200114\n","15 of 122 remaining\n","completed date:  20200115\n","16 of 122 remaining\n","completed date:  20200116\n","17 of 122 remaining\n","completed date:  20200117\n","18 of 122 remaining\n","completed date:  20200118\n","19 of 122 remaining\n","completed date:  20200119\n","20 of 122 remaining\n","completed date:  20200120\n","21 of 122 remaining\n","completed date:  20200121\n","22 of 122 remaining\n","completed date:  20200122\n","23 of 122 remaining\n","completed date:  20200123\n","24 of 122 remaining\n","completed date:  20200124\n","25 of 122 remaining\n","completed date:  20200125\n","26 of 122 remaining\n","completed date:  20200126\n","27 of 122 remaining\n","completed date:  20200127\n","28 of 122 remaining\n","completed date:  20200128\n","29 of 122 remaining\n","completed date:  20200129\n","30 of 122 remaining\n","completed date:  20200130\n","31 of 122 remaining\n","completed date:  20200131\n","32 of 122 remaining\n","completed date:  20200201\n","33 of 122 remaining\n","completed date:  20200202\n","34 of 122 remaining\n","completed date:  20200203\n","35 of 122 remaining\n","completed date:  20200204\n","36 of 122 remaining\n","completed date:  20200205\n","37 of 122 remaining\n","completed date:  20200206\n","38 of 122 remaining\n","completed date:  20200207\n","39 of 122 remaining\n","completed date:  20200208\n","40 of 122 remaining\n","completed date:  20200209\n","41 of 122 remaining\n","completed date:  20200210\n","42 of 122 remaining\n","completed date:  20200211\n","43 of 122 remaining\n","completed date:  20200212\n","44 of 122 remaining\n","completed date:  20200213\n","45 of 122 remaining\n","completed date:  20200214\n","46 of 122 remaining\n","completed date:  20200215\n","47 of 122 remaining\n","completed date:  20200216\n","48 of 122 remaining\n","completed date:  20200217\n","49 of 122 remaining\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ft4PUiHIkDLH","colab_type":"code","colab":{}},"source":["#API to get the articles with the SPECIFIC NZ actors I need as Actor1name does not give the auctual name as I assume...\n","#gets the data for during the corona virus period average run time 30 - 35 mins for 4 months of records hourly \n","import pandas as pd\n","import csv \n","import timeit\n","start = timeit.default_timer()\n","#API I want to download CSV file from includes my key words\n","#Looks like I need to get each actor individually so do 3 invidual api calls RIP :(\n","NZ_Corona_URL = \"https://api.gdeltproject.org/api/v2/doc/doc?format=html&query=(Jacinda%20Ardern)sourcecountry:NZ&mode=artlist&maxrecords=250&format=csv&sort=hybridrel\"\n","#\"https://api.gdeltproject.org/api/v2/doc/doc?format=html&query=(%22David%20Clark%22%20OR%20%22Jacinda%20Ardern%22%20OR%20%22Ashley%20Bloomfield%22)sourcecountry:NZ&mode=artlist&maxrecords=250&format=csv&sort=hybridrel\"\n","#Create all the times in 24 hours\n","time_loop = create_time()\n","#start a counter to see remaining days left to process\n","count = 0\n","#get the date range for the specific dates we want in API format\n","date_range = get_date_range('2019-12-31', '2020-04-30') # 2019-12-31\n","#set the file name we want our condensed file as \n","filename = \"Jacinda_Ardern_All_files.csv\"\n","#just a simple variable to see how many days in total we are doing to see how many left\n","total = len(date_range)\n","#generate an empty csv file as to not error at start\n","df = pd.DataFrame(list())\n","df.to_csv(filename)\n","\n","#loop through all dates calling the API on every date combining each time\n","for date in date_range:\n","  Api_Hourly_call(date, time_loop, NZ_Corona_URL, filename)\n","  print(\"completed date: \", date)\n","  count = count + 1\n","  print(str(count) + \" of \" + str(total) + \" remaining\")\n","stop = timeit.default_timer()\n","print('Time taken to download all files: ', stop - start) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NaBv2zHPKnJH","colab_type":"text"},"source":["#All data from range, dont need to use the API for this."]},{"cell_type":"markdown","metadata":{"id":"PKkJA_VhXIso","colab_type":"text"},"source":["moving on to fetching gdelt data from table "]},{"cell_type":"code","metadata":{"id":"XAnfqf8ncnY5","colab_type":"code","colab":{}},"source":["#James Atlas code with dates changed - just downloads all the gdelt data I need from the tables\n","from concurrent.futures import ProcessPoolExecutor\n","from datetime import date, timedelta\n","import pandas as pd\n","import gdelt\n","import os\n","\n","# set up gdeltpyr for version 2\n","gd = gdelt.gdelt(version=2)\n","\n","# multiprocess the query\n","e = ProcessPoolExecutor()\n","\n","\n","# generic functions to pull and write data to disk based on date\n","def get_filename(x):\n","  date = x.strftime('%Y%m%d')\n","  return \"{}_gdeltdata.csv\".format(date)\n","\n","def intofile(filename):\n","    try:\n","        if not os.path.exists(filename):\n","          date = filename.split(\"_\")[0]\n","          d = gd.Search(date, table='events',coverage=False) #not updata at 15mins\n","          d.to_csv(filename,encoding='utf-8',index=False)\n","    except:\n","        print(\"Error occurred\")\n","\n","# pull the data from gdelt into multi files; this may take a long time\n","dates = [get_filename(x) for x in pd.date_range('2019-12-31','2020-04-30')]\n","\n","results = list(e.map(intofile,dates))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2xgqhqpew46","colab_type":"text"},"source":["Combine all the Gdeltdata into a single CSV file"]},{"cell_type":"code","metadata":{"id":"6gTKRpoCewW3","colab_type":"code","colab":{}},"source":["from pyspark.sql import SQLContext\n","file_format = \"*_gdeltdata\"\n","sqlContext = SQLContext(sc)\n","data_urls = sqlContext.read.option(\"header\", \"true\").csv(dates)\n","data_urls = data_urls[['Actor1Name','Actor2Name','Actor1CountryCode', 'EventCode']]\n","data_urls.write.csv('before_data_unfiltered.csv') #this merges it in to a folder of 5 csv which acts as a single csv file"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVjB7_V6Yysb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Lyvftr0TF8_","colab_type":"text"},"source":["Do a join to merge NZ_Actors_By_Name_during_coronavirus and coronavirus_records_unfiltered This will give a table that contains all the records that are about one of these individuals and related to corona virus.\n"]},{"cell_type":"code","metadata":{"id":"ccI-HBMOTOtX","colab_type":"code","colab":{}},"source":["data_frame_virus = pd.read_csv('Coronavirus_records_unfiltered.csv')\n","data_frame_actors = pd.read_csv('NZ_Actors_By_Name_during_coronavirus.csv')\n","display(data_frame_virus)\n","display(data_frame_actors)\n"],"execution_count":0,"outputs":[]}]}